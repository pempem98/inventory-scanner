import os
import json
import time
import logging
import pandas as pd
from typing import Dict, List, Any, Optional
from collections import defaultdict

# Import c√°c module ƒë√£ ƒë∆∞·ª£c t√πy ch·ªânh
from .DatabaseManager import DatabaseManager
from .GoogleSheetDownloader import GoogleSheetDownloader
from .TelegramNotifier import TelegramNotifier

logger = logging.getLogger(__name__)

class InventoryScanner:
    """
    Qu·∫£n l√Ω lu·ªìng c√¥ng vi·ªác ch√≠nh: t·∫£i, so s√°nh, v√† th√¥ng b√°o d·ªØ li·ªáu
    d·ª±a tr√™n c·∫•u h√¨nh t·ª´ c∆° s·ªü d·ªØ li·ªáu SQLite.
    """

    def __init__(self, bot_token: str, proxies: Optional[Dict[str, str]] = None):
        """
        Kh·ªüi t·∫°o InventoryScanner.
        """
        db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "app.db")
        self.db_manager = DatabaseManager(db_file=db_path)
        self.proxies = proxies
        if bot_token:
            self.notifier = TelegramNotifier(bot_token=bot_token, proxies=self.proxies)
        else:
            self.notifier = None
            logger.warning("Kh√¥ng c√≥ BOT_TOKEN, s·∫Ω kh√¥ng c√≥ th√¥ng b√°o n√†o ƒë∆∞·ª£c g·ª≠i.")

    def _find_header_and_columns(self, df: pd.DataFrame, config: dict, mappings: List[Dict]) -> Optional[Dict[str, Any]]:
        """
        T·ª± ƒë·ªông t√¨m h√†ng header v√† v·ªã tr√≠ c·ªßa t·∫•t c·∫£ c√°c c·ªôt ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong danh s√°ch `mappings`.

        Args:
            df: DataFrame ch·ª©a d·ªØ li·ªáu t·ª´ file ngu·ªìn.
            config: Dictionary ch·ª©a th√¥ng tin c·∫•u h√¨nh c·ªßa d·ª± √°n.
            mappings: Danh s√°ch c√°c dictionary, m·ªói c√°i ch·ª©a th√¥ng tin c·ªßa m·ªôt ColumnMapping.

        Returns:
            M·ªôt dictionary ch·ª©a th√¥ng tin v·ªÅ header v√† v·ªã tr√≠ c√°c c·ªôt, ho·∫∑c None n·∫øu th·∫•t b·∫°i.
        """
        if not mappings:
            logger.error(f"D·ª± √°n {config['project_name']} kh√¥ng c√≥ c·∫•u h√¨nh c·ªôt (column mappings) n√†o.")
            return None

        identifier_map = next((m for m in mappings if m.get('is_identifier')), None)
        if not identifier_map:
            logger.error(f"D·ª± √°n {config['project_name']} kh√¥ng c√≥ c·ªôt n√†o ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† 'is_identifier: true'.")
            return None

        header_row_idx = -1
        config_header_idx = config.get('header_row_index')
        if config_header_idx and 0 < int(config_header_idx) <= len(df):
            header_row_idx = int(config_header_idx) - 1
        else:
            try:
                identifier_aliases = {str(alias).lower() for alias in json.loads(identifier_map.get('aliases', '[]'))}
                if not identifier_aliases:
                    logger.error(f"C·ªôt ƒë·ªãnh danh '{identifier_map['internal_name']}' kh√¥ng c√≥ 'aliases' n√†o ƒë∆∞·ª£c c·∫•u h√¨nh.")
                    return None

                for i, row in df.head(10).iterrows():
                    row_values = {str(val).strip().lower() for val in row.dropna().values}
                    if not identifier_aliases.isdisjoint(row_values):
                        header_row_idx = i
                        break
            except json.JSONDecodeError:
                logger.error(f"L·ªói JSON trong 'aliases' c·ªßa c·ªôt ƒë·ªãnh danh cho d·ª± √°n {config['project_name']}.")
                return None

        if header_row_idx == -1:
            logger.error(f"Kh√¥ng th·ªÉ t·ª± ƒë·ªông t√¨m th·∫•y h√†ng header cho d·ª± √°n {config['project_name']}.")
            return None

        def normalize_column_name(name: str):
            normalized_name = str(name).strip().lower() \
                .replace(' ', '').replace('\n', '').replace(')', '').replace('(', '') \
                .replace('&', '+').replace('v√†', '+').replace(',', '+')
            return normalized_name

        header_content = [normalize_column_name(h) for h in df.iloc[header_row_idx].tolist()]

        column_indices = {}
        for mapping in mappings:
            internal_key = mapping['internal_name']
            col_idx = None
            try:
                aliases = [normalize_column_name(alias) for alias in json.loads(mapping.get('aliases', '[]'))]
                for alias in aliases:
                    try:
                        col_idx = header_content.index(alias)
                        break
                    except ValueError:
                        continue
                column_indices[internal_key] = col_idx
            except json.JSONDecodeError:
                logger.error(f"L·ªói JSON trong 'aliases' c·ªßa c·ªôt '{internal_key}' cho d·ª± √°n {config['project_name']}.")
                column_indices[internal_key] = None

        identifier_key_name = identifier_map['internal_name']
        if column_indices.get(identifier_key_name) is None:
            logger.error(f"Kh√¥ng t√¨m th·∫•y c·ªôt ƒë·ªãnh danh '{identifier_key_name}' trong header c·ªßa d·ª± √°n {config['project_name']}.")
            return None

        logger.info(f"ƒê√£ x√°c ƒë·ªãnh header ·ªü d√≤ng {header_row_idx + 1}. C√°c ch·ªâ s·ªë c·ªôt: {column_indices}")

        return {
            "header_row_idx": header_row_idx,
            "identifier_key": identifier_key_name,
            "column_indices": column_indices,
            "header": header_content
        }

    def _normalize_and_validate_key(self, key: Any, prefixes: Optional[List[str]]) -> Optional[str]:
        if not isinstance(key, (str, int, float)): return None
        clean_key = str(key).strip().upper()
        if not clean_key: return None
        if not prefixes: return clean_key
        for prefix in prefixes:
            if clean_key.startswith(prefix.upper()):
                return clean_key
        return None

    def _extract_snapshot_data(self, data_df: pd.DataFrame, color_df: pd.DataFrame, header_info: dict, config: dict) -> Dict[str, Any]:
        """
        Tr√≠ch xu·∫•t d·ªØ li·ªáu snapshot d·ª±a tr√™n c·∫•u tr√∫c header_info linh ho·∫°t.
        """
        snapshot_data = {}
        identifier_key = header_info['identifier_key']
        column_indices = header_info['column_indices']
        identifier_col_idx = column_indices[identifier_key]

        invalid_colors_json = config.get('invalid_colors', '[]')
        invalid_colors = {c.lower() for c in json.loads(invalid_colors_json)}

        data_rows_df = data_df.iloc[header_info['header_row_idx'] + 1:]
        color_rows_df = color_df.iloc[header_info['header_row_idx'] + 1:]

        prefixes_json = config.get('key_prefixes')
        valid_prefixes = json.loads(prefixes_json) if prefixes_json else None

        for index, row in data_rows_df.iterrows():
            raw_key = row.iloc[identifier_col_idx]
            valid_key = self._normalize_and_validate_key(raw_key, valid_prefixes)

            if valid_key:
                try:
                    cell_color = color_rows_df.loc[index].iloc[identifier_col_idx]
                    if cell_color and cell_color.lower() in invalid_colors:
                        logger.info(f"B·ªè qua key '{valid_key}' do c√≥ m√†u kh√¥ng h·ª£p l·ªá: {cell_color}")
                        continue
                except (KeyError, IndexError):
                    pass

                row_data = {}
                for key, col_idx in column_indices.items():
                    if key == identifier_key or col_idx is None:
                        continue

                    value = row.iloc[col_idx]
                    row_data[key] = str(value) if pd.notna(value) else None

                snapshot_data[valid_key] = row_data

        return snapshot_data

    def _compare_snapshots(self, new_snapshot: Dict, old_snapshot: Dict) -> Dict[str, List]:
        """
        So s√°nh hai snapshot, bao g·ªìm t·∫•t c·∫£ c√°c tr∆∞·ªùng d·ªØ li·ªáu (price, policy, v.v.).
        """
        new_keys = set(new_snapshot.keys())
        old_keys = set(old_snapshot.keys())

        added = sorted(list(new_keys - old_keys))
        removed = sorted(list(old_keys - new_keys))

        changed = []
        common_keys = new_keys.intersection(old_keys)
        for key in common_keys:
            old_data = old_snapshot.get(key, {})
            new_data = new_snapshot.get(key, {})

            all_fields = set(old_data.keys()) | set(new_data.keys())

            for field in all_fields:
                old_value = old_data.get(field)
                new_value = new_data.get(field)

                old_is_nan = pd.isna(old_value)
                new_is_nan = pd.isna(new_value)

                if old_is_nan and new_is_nan:
                    continue

                if old_value != new_value:
                    changed.append({
                        "key": key,
                        "field": field,
                        "old": old_value,
                        "new": new_value
                    })

        return {'added': added, 'removed': removed, 'changed': changed}

    def run(self):
        logger.info("="*50)
        logger.info("B·∫ÆT ƒê·∫¶U PHI√äN L√ÄM VI·ªÜC M·ªöI")

        active_configs = self.db_manager.get_active_configs()
        if not active_configs:
            logger.warning("Kh√¥ng c√≥ c·∫•u h√¨nh n√†o ƒëang ho·∫°t ƒë·ªông trong database. K·∫øt th√∫c.")
            return

        all_individual_results = []
        for config_row in active_configs:
            config = dict(config_row)
            agent_name = config['agent_name']
            project_name = config['project_name']
            config_id = config['id']

            print("="*20)
            print(f"‚ñ∂Ô∏è  ƒêang x·ª≠ l√Ω: {agent_name} - {project_name} (ID: {config_id})")

            try:
                mappings = self.db_manager.get_column_mappings(config_id)
                downloader = GoogleSheetDownloader(
                    spreadsheet_id=config.get('spreadsheet_id'),
                    html_url=config.get('html_url'),
                    gid=config['gid'],
                    proxies=self.proxies
                )
                current_df, color_df, download_url = downloader.download()

                if current_df is None or color_df is None or current_df.empty:
                    logger.error(f"Kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu ho·∫∑c m√†u s·∫Øc cho ID {config_id}.")
                    continue

                header_info = self._find_header_and_columns(current_df, config, mappings)
                if not header_info:
                    logger.error(f"Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c header/c·ªôt cho ID {config_id}.")
                    continue

                new_snapshot = self._extract_snapshot_data(current_df, color_df, header_info, config)

                old_snapshot = self.db_manager.get_latest_snapshot(config_id)

                if old_snapshot is not None:
                    comparison = self._compare_snapshots(new_snapshot, old_snapshot)
                    print(f"    -> So s√°nh ho√†n t·∫•t: {len(comparison['added'])} th√™m, {len(comparison['removed'])} b√°n, {len(comparison['changed'])} ƒë·ªïi.")
                else:
                    comparison = {'added': list(new_snapshot.keys()), 'removed': [], 'changed': []}
                    print("    -> L·∫ßn ƒë·∫ßu ch·∫°y, ghi nh·∫≠n to√†n b·ªô l√† cƒÉn m·ªõi.")

                if comparison.get('added') or comparison.get('removed') or comparison.get('changed'):
                    all_individual_results.append({
                        'agent_name': agent_name,
                        'project_name': project_name,
                        'telegram_chat_id': config['telegram_chat_id'],
                        'comparison': comparison
                    })

                self.db_manager.add_snapshot(config_id, new_snapshot)
                print(f"    -> ƒê√£ l∆∞u snapshot m·ªõi v·ªõi {len(new_snapshot)} keys.")

            except Exception as e:
                logger.exception(f"L·ªói nghi√™m tr·ªçng khi x·ª≠ l√Ω c·∫•u h√¨nh ID {config_id}: {e}")
                print(f"    ‚ùå L·ªói: {e}. Ki·ªÉm tra runtime.log ƒë·ªÉ bi·∫øt chi ti·∫øt.")

        print("="*20)
        print("üîÑ ƒêang t·ªïng h·ª£p v√† gom nh√≥m k·∫øt qu·∫£...")
        aggregated_results = defaultdict(lambda: {'added': [], 'removed': [], 'changed': [], 'telegram_chat_id': None})

        for result in all_individual_results:
            key = (result['agent_name'], result['project_name'])

            aggregated_results[key]['added'].extend(result['comparison']['added'])
            aggregated_results[key]['removed'].extend(result['comparison']['removed'])
            aggregated_results[key]['changed'].extend(result['comparison']['changed'])
            if not aggregated_results[key]['telegram_chat_id']:
                aggregated_results[key]['telegram_chat_id'] = result['telegram_chat_id']

        print("üöÄ ƒêang g·ª≠i c√°c th√¥ng b√°o t·ªïng h·ª£p...")
        if not self.notifier:
            print("    -> B·ªè qua v√¨ kh√¥ng c√≥ BOT_TOKEN.")
            return

        for (agent_name, project_name), data in aggregated_results.items():
            chat_id = data['telegram_chat_id']
            if not chat_id:
                continue

            final_result_for_message = {
                'agent_name': agent_name,
                'project_name': project_name,
                'comparison': {
                    'added': data['added'],
                    'removed': data['removed'],
                    'changed': data['changed']
                }
            }

            message = self.notifier.format_message(final_result_for_message)

            if message:
                print(f"    -> G·ª≠i th√¥ng b√°o cho: {agent_name} - {project_name}")
                self.notifier.send_message(chat_id, message)
                time.sleep(3)

        self.db_manager.close()
        print("="*20)
        print("‚úÖ Ho√†n th√†nh t·∫•t c·∫£ c√°c t√°c v·ª•.")


if __name__ == "__main__":
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    proxies = {
        'http': 'http://rb-proxy-apac.bosch.com:8080',
        'https': 'http://rb-proxy-apac.bosch.com:8080'
    }
    if not bot_token:
        print("L·ªói: Vui l√≤ng thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng TELEGRAM_BOT_TOKEN.")
    else:
        manager = InventoryScanner(bot_token=bot_token, proxies=None)
        manager.run()
